{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# to split text into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tasks for Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_raw = '/../../../../blobdata/raw/news.ndjson'\n",
    "path_save_tasks = '../raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_save(json_file, output_dir, num_items, num_skip=0, condition=False):\n",
    "    \"\"\"Transforms json file with raw data into multiple tasks for Label Studio\"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(json_file, 'r') as f_in:\n",
    "        # Skip the first # lines\n",
    "        for _ in range(num_skip):\n",
    "            if not f_in.readline():\n",
    "                return  # Exit if there are less than 500 lines\n",
    "            \n",
    "        count = 0\n",
    "        while count <= num_items:\n",
    "            line = f_in.readline()\n",
    "\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            item = json.loads(line)\n",
    "\n",
    "            concatenated_text = item.get('+header:en', '') + '. ' + item.get('+abstract:en', '')\n",
    "\n",
    "            if condition:\n",
    "                if 'acquire' in concatenated_text:\n",
    "                    transformed_item = {\n",
    "                        \"data\": {\n",
    "                            \"text\": concatenated_text.strip(),\n",
    "                            \"meta_info\": {\n",
    "                                \"timestamp\": str(item['published_at+timestamp']),\n",
    "                                \"location\": item.get('+domain_n', '')\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    output_file = os.path.join(output_dir, f'entity_{count + num_skip}.json')\n",
    "                    with open(output_file, 'w') as f_out:\n",
    "                        json.dump(transformed_item, f_out, indent=2)\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                transformed_item = {\n",
    "                    \"data\": {\n",
    "                        \"text\": concatenated_text.strip(),\n",
    "                        \"meta_info\": {\n",
    "                            \"timestamp\": str(item['published_at+timestamp']),\n",
    "                            \"location\": item.get('+domain_n', '')\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                output_file = os.path.join(output_dir, f'entity_{count}.json')\n",
    "                with open(output_file, 'w') as f_out:\n",
    "                    json.dump(transformed_item, f_out, indent=2)\n",
    "\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_and_save(path_to_data_raw, path_save_tasks, num_items=100, num_skip=550, condition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_annotations = '../benchmark_data/annotations.json'\n",
    "path_save_split = '../benchmark_data/train_test_split/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from json to pd.DataFrame\n",
    "\n",
    "def split_text_into_sentences(row):\n",
    "    \"\"\"Splits text into multiple sentences.\"\"\"\n",
    "\n",
    "    # split text into sentences\n",
    "    sentences = sent_tokenize(row['text'])\n",
    "    result = pd.DataFrame({'text': sentences})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def transform_json(path):\n",
    "    \"\"\"\n",
    "    Transforms json into pd.DataFrame.\n",
    "    Args:\n",
    "        path : string that contains path to the json file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for item in data:\n",
    "\n",
    "        text = item['data']['text']\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'text': texts})\n",
    "    df = pd.concat([split_text_into_sentences(row) for _, row in df.iterrows()], ignore_index=True)\n",
    "    df = df.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_json(path_to_data_annotations)\n",
    "train_df, test_df = train_test_split(df, test_size=0.33, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zelensky is Pinocchio, officer tells The New Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Ukrainian officer interviewed by The New Yor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SmartBear Names SVP of Growth Marketing to Sca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kevin Foster brings 25+ years of marketing exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kevin comes to SmartBear from the startup High...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>Top End Energy Advances Low-Carbon Vision.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Top End Energy Limited (AU:TEE) has released a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Top End Energy Limited, an Australian energy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>Azelis Expands Its Portfolio in Germany With t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Regulatory News: Azelis (Brussels:AZE), a lead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Zelensky is Pinocchio, officer tells The New Y...\n",
       "1    A Ukrainian officer interviewed by The New Yor...\n",
       "2    SmartBear Names SVP of Growth Marketing to Sca...\n",
       "3    Kevin Foster brings 25+ years of marketing exp...\n",
       "4    Kevin comes to SmartBear from the startup High...\n",
       "..                                                 ...\n",
       "881         Top End Energy Advances Low-Carbon Vision.\n",
       "882  Top End Energy Limited (AU:TEE) has released a...\n",
       "883  Top End Energy Limited, an Australian energy c...\n",
       "884  Azelis Expands Its Portfolio in Germany With t...\n",
       "885  Regulatory News: Azelis (Brussels:AZE), a lead...\n",
       "\n",
       "[886 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(path_save_split + 'train.csv', index=False)\n",
    "test_df.to_csv(path_save_split + 'test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
