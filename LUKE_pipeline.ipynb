{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devuser1/miniconda3/envs/python3_11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "# to split text into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# LUKE model\n",
    "from transformers import LukeTokenizer, LukeForEntityPairClassification, LukeForEntitySpanClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../benchmark_data/'\n",
    "path_to_models = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['model_name', 'macro_precision', 'macro_recall', 'macro_f1', 'p_works_at', 'p_partners_with', 'p_acquired_by', 'r_works_at', 'r_partners_with', 'r_acquired_by', 'f1_works_at', 'f1_partners_with', 'f1_acquired_by'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoSentenceError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_sentence(text, span):\n",
    "    \"\"\"Returns sentence for the given span\"\"\"\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        start_idx = text.find(sentence)\n",
    "        end_idx = start_idx + len(sentence)\n",
    "        if span[0] >= start_idx and span[1] <= end_idx:\n",
    "            return sentence, start_idx\n",
    "    \n",
    "    raise NoSentenceError(\"entities are not in one sentence\")\n",
    "\n",
    "def adjust_span(span, offset):\n",
    "    \"\"\"Adjusts span\"\"\"\n",
    "\n",
    "    return (span[0] - offset, span[1] - offset)\n",
    "\n",
    "def transform_json_re(path, additional_rows=100):\n",
    "    \"\"\"Transfomrs json file into dataframe with entity relations, adds also rows with no relation\"\"\"\n",
    "\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    entity_pairs = []\n",
    "    entity_spans_pairs = []\n",
    "    relation_labels = []\n",
    "\n",
    "    total_additional_rows = 0\n",
    "\n",
    "    for item in data:\n",
    "        item_entities = {}\n",
    "        text = item['data']['text']\n",
    "        annotations = item['annotations'][0]['result']\n",
    "\n",
    "        related_pairs = set()\n",
    "        for annotation in annotations:\n",
    "\n",
    "            # first create list of entities in this text\n",
    "            if annotation['type'] == 'labels':\n",
    "                entity_span = (annotation['value']['start'], annotation['value']['end'])\n",
    "                entity_text = annotation['value']['text']\n",
    "                entity_id = annotation['id']\n",
    "                entity_label = annotation['value']['labels'][0]\n",
    "                item_entities[entity_id] = {'text': entity_text, 'span': entity_span, 'label': entity_label}\n",
    "\n",
    "            # create entity pairs according to annotations\n",
    "            elif annotation['type'] == 'relation':\n",
    "                from_entity = item_entities[annotation['from_id']]\n",
    "                to_entity = item_entities[annotation['to_id']]\n",
    "                try:\n",
    "                    label = annotation['labels'][0]\n",
    "                except KeyError:\n",
    "                    # if the labels is missing skip annotation\n",
    "                    continue\n",
    "\n",
    "                from_span = from_entity['span']\n",
    "                to_span = to_entity['span']\n",
    "\n",
    "                try: \n",
    "                    if from_span[0] < to_span[0]:\n",
    "                        sentence, offset = get_sentence(text, (from_span[0], to_span[1]))\n",
    "                    else:\n",
    "                        sentence, offset = get_sentence(text, (to_span[0], from_span[1]))\n",
    "                except NoSentenceError as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # adjust span of the entities according to the sentence\n",
    "                from_span_adjusted = adjust_span(from_span, offset)\n",
    "                to_span_adjusted = adjust_span(to_span, offset)\n",
    "\n",
    "                # add relation to the dataframe\n",
    "                texts.append(sentence)\n",
    "                entity_pairs.append((from_entity['text'], to_entity['text']))\n",
    "                entity_spans_pairs.append((from_span_adjusted, to_span_adjusted))\n",
    "                relation_labels.append(label)\n",
    "\n",
    "                # list of entity pairs that are actually related\n",
    "                related_pairs.add((from_entity['text'], to_entity['text']))\n",
    "\n",
    "        # create all possible pairs of the entities in this text\n",
    "        all_entities = list(item_entities.values())\n",
    "        all_pairs = list(combinations(all_entities, 2))\n",
    "\n",
    "        # entity pairs that does not have a relation\n",
    "        non_related_pairs = [(e1, e2) for e1, e2 in all_pairs if (e1['text'], e2['text']) not in related_pairs]\n",
    "\n",
    "        random.seed(4)\n",
    "        random.shuffle(non_related_pairs)\n",
    "        for non_related_pair in non_related_pairs:\n",
    "            if total_additional_rows >= additional_rows:\n",
    "                break\n",
    "\n",
    "            e1, e2 = non_related_pair\n",
    "            from_span = e1['span']\n",
    "            to_span = e2['span']\n",
    "\n",
    "            try: \n",
    "                if from_span[0] < to_span[0]:\n",
    "                    sentence, offset = get_sentence(text, (from_span[0], to_span[1]))\n",
    "                else:\n",
    "                    sentence, offset = get_sentence(text, (to_span[0], from_span[1]))\n",
    "            except NoSentenceError as e:\n",
    "                continue\n",
    "\n",
    "            from_span_adjusted = adjust_span(from_span, offset)\n",
    "            to_span_adjusted = adjust_span(to_span, offset)\n",
    "\n",
    "            texts.append(sentence)\n",
    "            entity_pairs.append((e1['text'], e2['text']))\n",
    "            entity_spans_pairs.append((from_span_adjusted, to_span_adjusted))\n",
    "            relation_labels.append('NIL')\n",
    "\n",
    "            total_additional_rows += 1\n",
    "\n",
    "    df = pd.DataFrame({'text': texts, 'entity_pairs': entity_pairs, 'entity_spans_pairs': entity_spans_pairs, 'label': relation_labels})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n",
      "entities are not in one sentence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entity_pairs</th>\n",
       "      <th>entity_spans_pairs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLX Networks Partners with GK3 Capital to Prov...</td>\n",
       "      <td>(FLX Networks, GK3 Capital)</td>\n",
       "      <td>((0, 12), (27, 38))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Informatica Expands Partnership with Google Cl...</td>\n",
       "      <td>(Informatica, Google Cloud)</td>\n",
       "      <td>((0, 11), (37, 49))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macy's has settled its proxy fight with Arkhou...</td>\n",
       "      <td>(Macy, Arkhouse)</td>\n",
       "      <td>((0, 4), (40, 48))</td>\n",
       "      <td>NIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atto, a leading provider of credit risk soluti...</td>\n",
       "      <td>(Atto, Fico)</td>\n",
       "      <td>((0, 4), (129, 133))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bosch and Randox: Strategic partnership brings...</td>\n",
       "      <td>(Bosch, Randox)</td>\n",
       "      <td>((0, 5), (10, 16))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>TCG World, the fast-growing and immersive Web3...</td>\n",
       "      <td>(TCG World, STYNGR)</td>\n",
       "      <td>((0, 9), (126, 132))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>TCG World, the fast-growing and immersive Web3...</td>\n",
       "      <td>(TCG World, Downtown)</td>\n",
       "      <td>((0, 9), (203, 211))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>TCG World, the fast-growing and immersive Web3...</td>\n",
       "      <td>(TCG World, STYNGR)</td>\n",
       "      <td>((320, 329), (331, 337))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>TCG World, the fast-growing and immersive Web3...</td>\n",
       "      <td>(TCG World, Downtown)</td>\n",
       "      <td>((320, 329), (343, 351))</td>\n",
       "      <td>partners_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Azelis Expands Its Portfolio in Germany With t...</td>\n",
       "      <td>(DBH Osthandelsgesellschaft mbH, Azelis)</td>\n",
       "      <td>((102, 132), (0, 6))</td>\n",
       "      <td>acquired_by</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    FLX Networks Partners with GK3 Capital to Prov...   \n",
       "1    Informatica Expands Partnership with Google Cl...   \n",
       "2    Macy's has settled its proxy fight with Arkhou...   \n",
       "3    Atto, a leading provider of credit risk soluti...   \n",
       "4    Bosch and Randox: Strategic partnership brings...   \n",
       "..                                                 ...   \n",
       "126  TCG World, the fast-growing and immersive Web3...   \n",
       "127  TCG World, the fast-growing and immersive Web3...   \n",
       "128  TCG World, the fast-growing and immersive Web3...   \n",
       "129  TCG World, the fast-growing and immersive Web3...   \n",
       "130  Azelis Expands Its Portfolio in Germany With t...   \n",
       "\n",
       "                                 entity_pairs        entity_spans_pairs  \\\n",
       "0                 (FLX Networks, GK3 Capital)       ((0, 12), (27, 38))   \n",
       "1                 (Informatica, Google Cloud)       ((0, 11), (37, 49))   \n",
       "2                            (Macy, Arkhouse)        ((0, 4), (40, 48))   \n",
       "3                                (Atto, Fico)      ((0, 4), (129, 133))   \n",
       "4                             (Bosch, Randox)        ((0, 5), (10, 16))   \n",
       "..                                        ...                       ...   \n",
       "126                       (TCG World, STYNGR)      ((0, 9), (126, 132))   \n",
       "127                     (TCG World, Downtown)      ((0, 9), (203, 211))   \n",
       "128                       (TCG World, STYNGR)  ((320, 329), (331, 337))   \n",
       "129                     (TCG World, Downtown)  ((320, 329), (343, 351))   \n",
       "130  (DBH Osthandelsgesellschaft mbH, Azelis)      ((102, 132), (0, 6))   \n",
       "\n",
       "             label  \n",
       "0    partners_with  \n",
       "1    partners_with  \n",
       "2              NIL  \n",
       "3    partners_with  \n",
       "4    partners_with  \n",
       "..             ...  \n",
       "126  partners_with  \n",
       "127  partners_with  \n",
       "128  partners_with  \n",
       "129  partners_with  \n",
       "130    acquired_by  \n",
       "\n",
       "[131 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re = transform_json_re(path_to_data + 'annotations.json')\n",
    "test_split = pd.read_csv(path_to_data + 'train_test_split/test.csv')\n",
    "\n",
    "test_df = pd.merge(df_re, test_split, how='inner', on='text')\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entity_pairs</th>\n",
       "      <th>entity_spans_pairs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Petrolicious' often-copied but unparalleled f...</td>\n",
       "      <td>[(Antoine Tessier, DRG)]</td>\n",
       "      <td>[((132, 147), (123, 126))]</td>\n",
       "      <td>[works_at]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(marketscreener.com) April 10, 2024 Accenture ...</td>\n",
       "      <td>[(Unlimited, Accenture), (Unlimited, Accenture)]</td>\n",
       "      <td>[((55, 64), (36, 45)), ((182, 191), (159, 168))]</td>\n",
       "      <td>[acquired_by, acquired_by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(marketscreener.com) DENVER, April 10, 2024 /P...</td>\n",
       "      <td>[(Dennis Pullin, DaVita Inc.)]</td>\n",
       "      <td>[((158, 171), (60, 71))]</td>\n",
       "      <td>[works_at]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(marketscreener.com) John Marshall Bank , subs...</td>\n",
       "      <td>[(Sean Biehl, John Marshall Bank), (John Marsh...</td>\n",
       "      <td>[((126, 136), (21, 39)), ((21, 39), (126, 136)...</td>\n",
       "      <td>[works_at, NIL, NIL, NIL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(marketscreener.com) The following is a round-...</td>\n",
       "      <td>[(Sylvanite Gold Tailings, Fulcrum Metals Cana...</td>\n",
       "      <td>[((315, 338), (233, 258))]</td>\n",
       "      <td>[acquired_by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Unveiling its more powerful next-generation AI...</td>\n",
       "      <td>[(Pat Gelsinger, Intel)]</td>\n",
       "      <td>[((91, 104), (81, 86))]</td>\n",
       "      <td>[works_at]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>VYRE Network announces a groundbreaking partne...</td>\n",
       "      <td>[(VYRE Network, Triad Entertainment Network)]</td>\n",
       "      <td>[((0, 12), (57, 84))]</td>\n",
       "      <td>[partners_with]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Weave has plans to undertake an extensive eigh...</td>\n",
       "      <td>[(Weave, KKR)]</td>\n",
       "      <td>[((93, 98), (239, 242))]</td>\n",
       "      <td>[partners_with]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Wednesday 10 April, 2024 Bosch and Randox Labo...</td>\n",
       "      <td>[(Bosch, Randox Laboratories Ltd.), (Bosch, Bo...</td>\n",
       "      <td>[((25, 30), (35, 59)), ((25, 30), (264, 269))]</td>\n",
       "      <td>[partners_with, NIL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>With the acquisition of YTH Sports, Unrivaled ...</td>\n",
       "      <td>[(YTH Sports, Unrivaled Sports), (YTH Sports, ...</td>\n",
       "      <td>[((24, 34), (36, 52)), ((103, 113), (77, 93))]</td>\n",
       "      <td>[acquired_by, acquired_by]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   \"Petrolicious' often-copied but unparalleled f...   \n",
       "1   (marketscreener.com) April 10, 2024 Accenture ...   \n",
       "2   (marketscreener.com) DENVER, April 10, 2024 /P...   \n",
       "3   (marketscreener.com) John Marshall Bank , subs...   \n",
       "4   (marketscreener.com) The following is a round-...   \n",
       "..                                                ...   \n",
       "91  Unveiling its more powerful next-generation AI...   \n",
       "92  VYRE Network announces a groundbreaking partne...   \n",
       "93  Weave has plans to undertake an extensive eigh...   \n",
       "94  Wednesday 10 April, 2024 Bosch and Randox Labo...   \n",
       "95  With the acquisition of YTH Sports, Unrivaled ...   \n",
       "\n",
       "                                         entity_pairs  \\\n",
       "0                            [(Antoine Tessier, DRG)]   \n",
       "1    [(Unlimited, Accenture), (Unlimited, Accenture)]   \n",
       "2                      [(Dennis Pullin, DaVita Inc.)]   \n",
       "3   [(Sean Biehl, John Marshall Bank), (John Marsh...   \n",
       "4   [(Sylvanite Gold Tailings, Fulcrum Metals Cana...   \n",
       "..                                                ...   \n",
       "91                           [(Pat Gelsinger, Intel)]   \n",
       "92      [(VYRE Network, Triad Entertainment Network)]   \n",
       "93                                     [(Weave, KKR)]   \n",
       "94  [(Bosch, Randox Laboratories Ltd.), (Bosch, Bo...   \n",
       "95  [(YTH Sports, Unrivaled Sports), (YTH Sports, ...   \n",
       "\n",
       "                                   entity_spans_pairs  \\\n",
       "0                          [((132, 147), (123, 126))]   \n",
       "1    [((55, 64), (36, 45)), ((182, 191), (159, 168))]   \n",
       "2                            [((158, 171), (60, 71))]   \n",
       "3   [((126, 136), (21, 39)), ((21, 39), (126, 136)...   \n",
       "4                          [((315, 338), (233, 258))]   \n",
       "..                                                ...   \n",
       "91                            [((91, 104), (81, 86))]   \n",
       "92                              [((0, 12), (57, 84))]   \n",
       "93                           [((93, 98), (239, 242))]   \n",
       "94     [((25, 30), (35, 59)), ((25, 30), (264, 269))]   \n",
       "95     [((24, 34), (36, 52)), ((103, 113), (77, 93))]   \n",
       "\n",
       "                         label  \n",
       "0                   [works_at]  \n",
       "1   [acquired_by, acquired_by]  \n",
       "2                   [works_at]  \n",
       "3    [works_at, NIL, NIL, NIL]  \n",
       "4                [acquired_by]  \n",
       "..                         ...  \n",
       "91                  [works_at]  \n",
       "92             [partners_with]  \n",
       "93             [partners_with]  \n",
       "94        [partners_with, NIL]  \n",
       "95  [acquired_by, acquired_by]  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = test_df.groupby('text').agg({\n",
    "    'entity_pairs': list,\n",
    "    'entity_spans_pairs': list,\n",
    "    'label': list\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# models' names\n",
    "model_name_ner = \"nk_LUKE_ner\"\n",
    "model_name_re = \"nk_LUKE_re\"\n",
    "model_name_luke_base = \"studio-ousia/luke-base\"\n",
    "\n",
    "task_entity_span = \"entity_span_classification\"\n",
    "task_pair_class = \"entity_pair_classification\"\n",
    "\n",
    "# models\n",
    "model_luke_ner = LukeForEntitySpanClassification.from_pretrained(path_to_models + model_name_ner)\n",
    "model_luke_re = LukeForEntityPairClassification.from_pretrained(path_to_models + model_name_re)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer_ner = LukeTokenizer.from_pretrained(model_name_luke_base, task=task_entity_span)\n",
    "tokenizer_re = LukeTokenizer.from_pretrained(model_name_luke_base, task=task_pair_class)\n",
    "\n",
    "id2label = {0: 'NIL', 1: 'works_at', 2: 'partners_with', 3: 'acquired_by'}\n",
    "label2id = {'NIL': 0, 'works_at': 1, 'partners_with': 2, 'acquired_by': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_start_pattern = re.compile(r'\\b\\w')\n",
    "word_end_pattern = re.compile(r'\\w\\b')\n",
    "\n",
    "def luke_ner_generate_entity_spans(text):\n",
    "\n",
    "    model_entity_spans = []\n",
    "\n",
    "    # word start and end positions to calculate spans \n",
    "    word_start_positions = [match.start() for match in word_start_pattern.finditer(text)]\n",
    "    word_end_positions = [match.end() for match in word_end_pattern.finditer(text)] \n",
    "\n",
    "    # all possible entity spans\n",
    "    # we consider only entity spans that are not longer then 6 words\n",
    "    for i, start_pos in enumerate(word_start_positions):\n",
    "        for end_pos in word_end_positions[i:i+6]:\n",
    "            model_entity_spans.append((start_pos, end_pos))\n",
    "\n",
    "    model_entity_spans.sort(key=lambda x: x[0])\n",
    "\n",
    "    return model_entity_spans\n",
    "\n",
    "\n",
    "def evaluation_luke_pipeline(name, ner_model, re_model, tokenizer_ner, tokenizer_re, test_df):\n",
    "    ner_model.eval()\n",
    "    re_model.eval()\n",
    "\n",
    "    class_labels = [1, 2, 3]\n",
    "\n",
    "    TP = {1:0, 2:0, 3:0}\n",
    "    FP = {1:0, 2:0, 3:0}\n",
    "    FN = {1:0, 2:0, 3:0}\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for row in test_df.itertuples():#tqdm(test_df.itertuples()):\n",
    "        text = row.text\n",
    "        true_pairs = row.entity_spans_pairs\n",
    "        true_labels = row.label\n",
    "\n",
    "        detected_entities = []\n",
    "        \n",
    "        # first perform NER to deted entities\n",
    "        entity_spans = luke_ner_generate_entity_spans(text)\n",
    "        ner_encoding = tokenizer_ner(text, entity_spans=entity_spans, truncation=True, return_tensors='pt')\n",
    "        ner_output = ner_model(**ner_encoding)\n",
    "\n",
    "        ner_logits = ner_output.logits\n",
    "\n",
    "        ner_predicted_class_indices = ner_logits.argmax(-1).squeeze().tolist()\n",
    "            \n",
    "        for span, predicted_class_idx in zip(entity_spans, ner_predicted_class_indices):\n",
    "            predicted_label = ner_model.config.id2label[predicted_class_idx]\n",
    "            if predicted_label in ['ORG', 'PER']:\n",
    "                detected_entities.append((span[0], span[1]))\n",
    "\n",
    "        if len(detected_entities) == 0:\n",
    "            for true_label in true_labels:\n",
    "                if true_label != 'NIL':\n",
    "                    FN[label2id[true_label]] = FN[label2id[true_label]] + 1\n",
    "                    \n",
    "            continue\n",
    "\n",
    "        all_pairs = list(permutations(detected_entities, 2))\n",
    "\n",
    "        \"\"\"\n",
    "        true_spans = set([item for sublist in true_pairs for item in sublist])\n",
    "        print('Entity spans: ', entity_spans)\n",
    "        print('True spans: ', true_pairs)\n",
    "        print('Intersection: ', set(entity_spans).intersection(true_spans))\n",
    "        print('Detected entities: ', detected_entities\n",
    "        \"\"\"\n",
    "\n",
    "        # classify all possible pairs of entities\n",
    "        for pair in all_pairs:\n",
    "            re_encoding = tokenizer_re(text, entity_spans=list(pair), return_tensors='pt')\n",
    "            re_output = re_model(**re_encoding)\n",
    "\n",
    "            re_logits = re_output.logits\n",
    "            predicted_class_idx = re_logits.argmax(-1).item()\n",
    "\n",
    "            #print('Pred', pair, re_model.config.id2label[predicted_class_idx])\n",
    "\n",
    "            # calculate the results\n",
    "            if pair in true_pairs:\n",
    "                inx = true_pairs.index(pair)\n",
    "                true_class = label2id[true_labels[inx]]\n",
    "                if predicted_class_idx == true_class:\n",
    "                    if true_class == 0:\n",
    "                        continue\n",
    "                    elif true_class in class_labels:\n",
    "                        TP[true_class] = TP[true_class] + 1\n",
    "                else:\n",
    "                    if true_class == 0:\n",
    "                        FP[predicted_class_idx] = FP[predicted_class_idx] + 1\n",
    "                    elif predicted_class_idx == 0:\n",
    "                        FN[true_class] = FN[true_class] + 1\n",
    "                    else:\n",
    "                        FN[true_class] = FN[true_class] + 1\n",
    "                        FP[predicted_class_idx] = FP[predicted_class_idx] + 1\n",
    "            else:\n",
    "                if predicted_class_idx == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    FP[predicted_class_idx] = FP[predicted_class_idx] + 1\n",
    "\n",
    "        \n",
    "    try:\n",
    "        for class_ in class_labels:\n",
    "            precision_class = TP[class_] / (TP[class_] + FP[class_])\n",
    "            recall_class = TP[class_] / (TP[class_] + FN[class_])\n",
    "\n",
    "            precisions.append(precision_class)\n",
    "            recalls.append(recall_class)\n",
    "            f1_scores.append(2 * precision_class * recall_class / (precision_class + recall_class))\n",
    "\n",
    "        macro_precision = sum(precisions) / len(class_labels)\n",
    "        macro_recall = sum(recalls) / len(class_labels)\n",
    "        macro_f1 = sum(f1_scores) / len(class_labels)\n",
    "\n",
    "        print(name)\n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        print(f\"Precision macro: {macro_precision}\")\n",
    "        print(f\"Recall macro: {macro_recall}\")\n",
    "        print(f\"F1 Score macro: {macro_f1}\")\n",
    "        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "\n",
    "    except ZeroDivisionError as e:\n",
    "        print(e)\n",
    "        macro_precision, macro_recall, macro_f1 = 0, 0, 0\n",
    "\n",
    "    return [name, macro_precision, macro_recall, macro_f1] + precisions + recalls + f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Precision macro: 0.5887799564270153\n",
      "Recall macro: 0.3829365079365079\n",
      "F1 Score macro: 0.45279790660225444\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pipeline',\n",
       " 0.5887799564270153,\n",
       " 0.3829365079365079,\n",
       " 0.45279790660225444,\n",
       " 0.9166666666666666,\n",
       " 0.5555555555555556,\n",
       " 0.29411764705882354,\n",
       " 0.4583333333333333,\n",
       " 0.35714285714285715,\n",
       " 0.3333333333333333,\n",
       " 0.611111111111111,\n",
       " 0.43478260869565216,\n",
       " 0.3125]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  \n",
    "Precision macro: 0.5887799564270153  \n",
    "Recall macro: 0.3829365079365079  \n",
    "F1 Score macro: 0.45279790660225444  \n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  \n",
    "\"\"\"\n",
    "\n",
    "result = evaluation_luke_pipeline('pipeline', model_luke_ner, model_luke_re, tokenizer_ner, tokenizer_re, grouped_df)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
